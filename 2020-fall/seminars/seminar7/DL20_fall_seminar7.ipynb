{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.5"
    },
    "colab": {
      "name": "DL20-fall-seminar7.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ysOZpjoNuk-K"
      },
      "source": [
        "# Сверточные сети в задачах компьютерного зрения\n",
        "\n",
        "**Разработчики: Артем Бабенко, Екатерина Глазкова**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8PrGzEGmuk-L"
      },
      "source": [
        "На этом семинаре необходимо будет (1) реализовать простейшую metric learning архитектуру на основе сиамской нейросети с Contrastive Loss и (2) обучить модель UNet для задачи сегментации изображения. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iA7mLqbRuk-M"
      },
      "source": [
        "# Metric Learning (0.5 баллов)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": true
        },
        "id": "BOllOSuduk-N"
      },
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from IPython.display import clear_output\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision.datasets as dsets\n",
        "import torchvision.transforms as transforms\n",
        "import torch.optim as optim\n",
        "from torch.utils.data.sampler import Sampler, BatchSampler\n",
        "from torch.nn.modules.loss import MSELoss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y1wumt0buk-T"
      },
      "source": [
        "Вам необходимо реализовать вычисление Contrastive Loss - одну из самых популярных функций потерь для metric learning. Contrastive Loss получает на вход пару векторов $x_i$ и $x_j$ (признаковые описания объектов $i$ и $j$, полученные нейросетью) и метку $y_{ij}$, причем $y_{ij} = 0$, если объекты \"похожи\" (принадлежат одному классу), и $y_{ij} = 1$, если объекты \"различны\" (принадлежат различным классам). Формально определим Contrastive Loss следующим образом:\n",
        "\n",
        "$$\n",
        "L(x_i, x_j, y_{ij}) = (1 - y_{ij})\\|x_i - x_j\\|^2 + y_{ij}max(0, m - \\|x_i - x_j\\|^2)\n",
        "$$\n",
        "\n",
        "где $m$ - гиперпараметр (его можно взять равным единице).\n",
        "\n",
        "Вместо того, чтобы формировать обучающее множество из всевозможных пар, можно поступить проще: будем пропускать батч из $N$ обучаюших изображений через сеть (тем самым получая соответствующие векторы $x$), а значение лосса вычислять как среднее значение функции $L$ на всех парах в этом батче. Тогда в обучении на каждом батче участвует $\\frac{N(N-1)}{2}$ пар, что существенно ускоряет сходимость на практике. Реализуйте предложенный вариант Contrastive Loss."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": true
        },
        "id": "-r1_6Mq4uk-U"
      },
      "source": [
        "class ContrastiveLoss(torch.nn.Module):\n",
        "    def __init__(self, margin=1.0):\n",
        "        super(ContrastiveLoss, self).__init__()\n",
        "        self.margin = margin\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        # x: batch_size x num_features\n",
        "        # y: batch_size\n",
        "        #<your code>"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r4GLh9jsuk-Y"
      },
      "source": [
        "В задачах metric learning, как правило, необходимо, чтобы количества \"положительных\" и \"отрицательных\" пар в обучении отличалось несильно. Поэтому в случае большого количества классов случайное формирование батчей неэффективно - в таком случае количество \"положительных\" пар очень мало. Поэтому будем формировать обучающие батчи размера $N$ следующим образом: будем брать $\\frac{N}{2}$ элементов из некоторого класса (они между собой будут формировать \"положительные пары\"), а оставшиеся $\\frac{N}{2}$ элементов будем брать случайно. Таким образом мы гарантируем, что в каждом обучающем батче будет достаточно \"положительных\" пар.\n",
        "\n",
        "Реализуйте предложенную логику в рамках Pytorch, реализовав собственный BatchSampler. Ваш самплер должен формировать каждый батч размера $N$ следующим образом: $\\frac{N}{2}$ объектов извлекаются из некоторого случайного класса, оставшиеся $\\frac{N}{2}$ объектов извлекаются случайно."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": true
        },
        "id": "mD-soGxyuk-Z"
      },
      "source": [
        "class ContrastiveSampler(BatchSampler):\n",
        "    def __init__(self, batch_size, num_classes, labels):\n",
        "        self.num_classes = num_classes\n",
        "        self.imgs_per_class = labels.size()[0] // num_classes\n",
        "        #<your code>\n",
        "\n",
        "    def __iter__(self):\n",
        "        num_yielded = 0\n",
        "        while num_yielded < (self.num_classes * self.imgs_per_class):\n",
        "            batch = []\n",
        "            #<your code>\n",
        "            num_yielded += self.batch_size\n",
        "            yield batch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yiArWNrHuk-c"
      },
      "source": [
        "В этом задании будем работать с небольшими изображениями одежды из датасета Fashion-MNIST."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": true
        },
        "id": "pG9gtImnuk-d"
      },
      "source": [
        "input_size = 784\n",
        "num_classes = 10\n",
        "batch_size = 256\n",
        "\n",
        "train_dataset = dsets.FashionMNIST(root='.', \n",
        "                                   train=True, \n",
        "                                   transform=transforms.ToTensor(),\n",
        "                                   download=True)\n",
        "\n",
        "test_dataset = dsets.FashionMNIST(root='.', \n",
        "                                  train=False, \n",
        "                                  transform=transforms.ToTensor())\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
        "                                           batch_sampler=ContrastiveSampler(batch_size=batch_size, num_classes=num_classes, labels=train_dataset.train_labels), \n",
        "                                           shuffle=False)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
        "                                          batch_sampler=ContrastiveSampler(batch_size=batch_size, num_classes=num_classes, labels=test_dataset.test_labels), \n",
        "                                          shuffle=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v08bnJfpuk-g"
      },
      "source": [
        "Реализуйте сеть несложной архитектуры, содержащую три сверточных слоя из 20 фильтров с макс-пулингом, а также два полносвязных слоя из 128 нейронов. Выход последнего слоя будет подаваться на вход Contrastive Loss."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": true
        },
        "id": "QC6vmcghuk-h"
      },
      "source": [
        "class Flatten(nn.Module):\n",
        "    def forward(self, x):\n",
        "        return x.view(x.size()[0], -1)\n",
        "\n",
        "class ContrastiveNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.cnn1 = nn.Sequential(\n",
        "            #<your code>\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        output = self.cnn1(x)\n",
        "        return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": true
        },
        "id": "bQyvGcKduk-k"
      },
      "source": [
        "contrastive_loss = ContrastiveLoss()\n",
        "\n",
        "def train_epoch(model, optimizer):\n",
        "    loss_log = []\n",
        "    model.train()\n",
        "    for batch_num, (data, target) in enumerate(train_loader):\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)        \n",
        "        loss = contrastive_loss(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        loss = loss.item()\n",
        "        loss_log.append(loss)\n",
        "    return loss_log   \n",
        "\n",
        "def test(model):\n",
        "    loss_log = []\n",
        "    model.eval()\n",
        "    for batch_num, (data, target) in enumerate(test_loader):    \n",
        "        output = model(data)\n",
        "        loss = contrastive_loss(output, target)\n",
        "        loss = loss.item()\n",
        "        loss_log.append(loss)\n",
        "    return loss_log\n",
        "\n",
        "def plot_history(train_history, val_history, title='loss'):\n",
        "    plt.figure()\n",
        "    plt.title('{}'.format(title))\n",
        "    plt.plot(train_history, label='train', zorder=1)    \n",
        "    points = np.array(val_history)\n",
        "    plt.scatter(points[:, 0], points[:, 1], marker='+', s=180, c='orange', label='val', zorder=2)\n",
        "    plt.xlabel('train steps')\n",
        "    plt.legend(loc='best')\n",
        "    plt.grid()\n",
        "    plt.show()\n",
        "    \n",
        "def train(model, opt, n_epochs):\n",
        "    train_log = []\n",
        "    val_log = []\n",
        "\n",
        "    for epoch in range(n_epochs):\n",
        "        print(\"Epoch {0} of {1}\".format(epoch, n_epochs))\n",
        "        train_loss = train_epoch(model, opt)\n",
        "        val_loss = test(model)\n",
        "        train_log.extend(train_loss)\n",
        "        steps = train_dataset.train_labels.shape[0] / batch_size\n",
        "        val_log.append((steps * (epoch + 1), np.mean(val_loss)))\n",
        "        clear_output()\n",
        "        plot_history(train_log, val_log)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": true
        },
        "id": "m1QpsRfpuk-q"
      },
      "source": [
        "model = ContrastiveNetwork()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x_vBnt52uk-0"
      },
      "source": [
        "Обучите сеть с параметрами, указанными ниже."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": true
        },
        "id": "j0plnAAouk-1"
      },
      "source": [
        "opt = torch.optim.Adam(model.parameters(), lr=0.0005)\n",
        "train(model, opt, 2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-JW-AX-Nuk-4"
      },
      "source": [
        "Извлеките векторные описания тестовых изображений (a.k.a эмбеддинги). У вас должно получиться 10000 128-мерных векторов."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": true
        },
        "id": "Cg9jSvm4uk-5"
      },
      "source": [
        "embeddings = #<your code>"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nofmBQ2luk-9"
      },
      "source": [
        "Код ниже демонстрирует поисковую выдачу для трех изображений-запросов. Выдача формируется на основе близости эмбеддингов."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": true
        },
        "id": "Zmq4bVNmuk-9"
      },
      "source": [
        "queryCount = 3\n",
        "queries = embeddings[:queryCount,:].data.numpy()\n",
        "database = embeddings[queryCount:,:].data.numpy()\n",
        "plt.figure(figsize=[15, 4.5])\n",
        "for i in range(queryCount):\n",
        "    results = np.argsort(np.sum((database-queries[i,:])**2, axis=1))[:10]\n",
        "    plt.subplot(queryCount, 11, i * 11 + 1)\n",
        "    plt.title(\"Query: %i\" % i)\n",
        "    plt.imshow(test_dataset.test_data[i].numpy().reshape([28, 28]), cmap='gray')\n",
        "    for k in range(10):\n",
        "        plt.subplot(queryCount, 11, i * 11 + k + 2)\n",
        "        plt.imshow(test_dataset.test_data[results[k]+queryCount].numpy().reshape([28, 28]), cmap='gray')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HPMg-415uk_B"
      },
      "source": [
        "# Сегментация изображений (0.5 баллов)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KVS9IYQiuk_B"
      },
      "source": [
        "Сегментация изображения - это задача классификации каждого пикселя. Например, вот так (пример из датасета Cityscapes Dataset): \n",
        "\n",
        "<img src=\"cityscapes_example.png\" width=\"600\" height=\"500\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LyTXh52xuk_D"
      },
      "source": [
        "В этом ноутбуке предлагается реализовать модель UNet для двухклассовой сегментации изображений."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9l6FELK4uk_D"
      },
      "source": [
        "from torch.optim.lr_scheduler import StepLR\n",
        "\n",
        "from tqdm import trange"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EXm4re8buk_G"
      },
      "source": [
        "from utils import WeizmannHorsesDataset, show_sample, plot_batch_with_results, plot_history, mean_accuracy, pixel_accuracy, mean_iou"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z5Y-e-Xquk_J"
      },
      "source": [
        "### Скачивание и чтение данных"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aSF24Qc1uk_K"
      },
      "source": [
        "В качестве датасета мы будем использовать датасет Weizmann Horse Database (источник: http://www.msri.org/m/people/members/eranb/ раздел - Horse Images), состоящий из черно-белых и цветных фотографий лошадей и разметки на два класса - foreground и background."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BQPzqyqGuk_K"
      },
      "source": [
        "#! wget http://www.msri.org/people/members/eranb/weizmann_horse_db.tar.gz -O data.tar.gz\n",
        "#! tar -xzf data.tar.gz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EKYCTQftuk_M"
      },
      "source": [
        "data_path = \"weizmann_horse_db\" \n",
        "dataset = WeizmannHorsesDataset(data_path, \"test\", img_shape = (40,60), color = \"rgb\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e5aae-3Juk_O",
        "outputId": "3ad6229e-cb66-4cc4-ddf0-51b065d8854e"
      },
      "source": [
        "show_sample(dataset, 0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1080x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p5M19xupuk_R"
      },
      "source": [
        "## Модель"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L4t2N10_uk_S"
      },
      "source": [
        "Схема модели UNet из оригинальной статьи (https://arxiv.org/abs/1505.04597 ):"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lgPV8VvJuk_S"
      },
      "source": [
        "<img src=\"UNet.png\" width=\"600\" height=\"500\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xxcDlcf6uk_T"
      },
      "source": [
        "На изображении выше синие блоки - карты признаков (feature maps). \n",
        "\n",
        "На вход модель принимает исходное изображение, результат модели - сегментация входного изображения (классификация каждого пикселя).\n",
        "\n",
        "Модель состоит из энкодера (на схеме слева) и декодера (справа).\n",
        "\n",
        "#### О реализации:\n",
        "\n",
        "Несколько одинаковых слоев сети, обрабатывающих feature map без изменения размера (например, повторение слоев Conv2d, BatchNorm2d, ReLU) реализованы в классе **ConvBlock**. На изображении один ConvBlock соответствует нескольким подряд идущим синим стрелкам.\n",
        "\n",
        "В реализации можно не делать crop при копировании feature map, а использовать padding, чтобы размеры карт признаков на одних и тех же уровнях декодера и энкодера совпадали.\n",
        "\n",
        "Для увеличения размера карты признаков при декодировании (**UpBlock**) вам могут понадобиться nn.ConvTranspose2d или nn.Upsample.\n",
        "\n",
        "В каждом следующем кодирующем/декодирующем слое **DownBlock**/**UpBlock** предполагается уменьшение/увеличение x-y размеров в $c_1$ раз и увеличение/уменьшение количества каналов в $c_2$ раз. Например, можно взять $c_1 = c_2 = 2$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s7qdkRJSuk_U"
      },
      "source": [
        "class ConvBlock(nn.Module):\n",
        "    '''\n",
        "       Convolutional Block, includes several sequential convolutional and activation layers.\n",
        "       Hint: include BatchNorm here\n",
        "    '''\n",
        "    def __init__(self, input_ch, output_ch, kernel_size, block_depth):\n",
        "        '''\n",
        "            input_ch - input channels num\n",
        "            output_ch - output channels num\n",
        "            kernel_size - kernel size for convolution layers\n",
        "            block_depth - number of convolution + activation repetitions\n",
        "        '''\n",
        "        super().__init__()\n",
        "        \n",
        "        # your code\n",
        "        # conv_list = \n",
        "\n",
        "        self.conv_net = nn.Sequential(*conv_list)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv_net(x)\n",
        "        return x\n",
        "    \n",
        "    \n",
        "class DownBlock(nn.Module):\n",
        "    '''\n",
        "        Encoding block, includes pooling (for shape reduction) and Convolutional Block (ConvBlock)\n",
        "    '''\n",
        "    def __init__(self, input_ch, output_ch, kernel_size, block_depth):\n",
        "        super().__init__()\n",
        "        \n",
        "        # your code\n",
        "        # self.layers = \n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.layers(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class UpBlock(nn.Module):\n",
        "    '''\n",
        "        Decoding block, includes upsampling and Convolutional Block (ConvBlock)\n",
        "    '''\n",
        "    def __init__(self, input_ch, output_ch, kernel_size, block_depth):\n",
        "        super().__init__()\n",
        "        \n",
        "        # your code\n",
        "\n",
        "    def forward(self, copied_input, lower_input):\n",
        "        '''\n",
        "            copied_input - feature map from one of the encoder layers\n",
        "            lower_input - feature map from previous decoder layer\n",
        "        '''\n",
        "        # your code\n",
        "        \n",
        "        return x\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ESt62aWeuk_X"
      },
      "source": [
        "class UNet(nn.Module):\n",
        "    def __init__(self, n_classes, feature_levels_num, input_ch_size, hidden_ch_size, block_depth, kernel_size = 3):\n",
        "        \"\"\"\n",
        "        Input:\n",
        "            n_classes - number of classes\n",
        "            feature_levels_num - number of down- and up- block levels\n",
        "            input_ch_size - input number of channels (1 for gray images, 3 for rgb)\n",
        "            hidden_ch_size - output number of channels of the first Convolutional Block (in the original paper - 32)\n",
        "            block_depth - number of convolutions + activations in one Convolutional Block\n",
        "            kernel_size - kernel size for all convolution layers\n",
        "        \"\"\"\n",
        "        super(UNet, self).__init__()\n",
        "        self.input_block = ConvBlock(input_ch_size, hidden_ch_size, 1, block_depth)\n",
        "        self.down_blocks = []\n",
        "        self.up_blocks = []\n",
        "        self.feature_levels_num = feature_levels_num\n",
        "        \n",
        "        \n",
        "        cur_ch_num = hidden_ch_size\n",
        "        for _ in range(feature_levels_num):\n",
        "            # your code\n",
        "            # fill self.down_blocks and self.up_blocks with DownBlock/UpBlock\n",
        "            # each DownBlock/UpBlock increase/decrease number of channels by 2 times\n",
        "        \n",
        "        self.down_blocks = nn.ModuleList(self.down_blocks)\n",
        "        self.up_blocks = nn.ModuleList(self.up_blocks)\n",
        "        self.output_block = ConvBlock(hidden_ch_size, n_classes, 1, block_depth)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.input_block(x)\n",
        "        \n",
        "        # your code\n",
        "        \n",
        "        x = self.output_block(x)\n",
        "        return x  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RlnS-6Yiuk_a"
      },
      "source": [
        "## Метрики качества"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kts-Pv3luk_b"
      },
      "source": [
        "Для сегментации изображения размера $H \\times W$ на множество классов $С$ могут быть рассмотрены следующие метрики:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W-F7GLffuk_b"
      },
      "source": [
        "$$Mean~IoU = \\frac{1}{|C|}\\sum_{i \\in C}\\frac{TP_i}{FP_i + TP_i + FN_i}$$\n",
        "\n",
        "$$Pixel~Accuracy = \\frac{\\sum_{i \\in C}TP_i}{H\\times W}$$\n",
        "\n",
        "$$Mean~Accuracy = \\sum_{i \\in C}\\frac{TP_i}{TP_i + FN_i}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AVBdF1t_uk_c"
      },
      "source": [
        "## Обучение"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MX2tKjrRuk_c"
      },
      "source": [
        "from collections import defaultdict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HE6bczIDuk_f"
      },
      "source": [
        "def train_epoch(model, optimizer, train_loader):\n",
        "    loss_log = []\n",
        "    model.train()\n",
        "    for _, (x_batch, y_batch) in zip(trange(len(train_loader)), train_loader):\n",
        "        optimizer.zero_grad()\n",
        "        prediction = model(x_batch)\n",
        "        loss = nn.CrossEntropyLoss()(prediction, y_batch.squeeze())\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        loss_log.append(loss.item())\n",
        "    return loss_log\n",
        "\n",
        "def test(model, test_loader, train_loader):\n",
        "    '''\n",
        "        Computes metrics on both train and test, loss on test\n",
        "    '''\n",
        "    history = defaultdict(list)\n",
        "    model.eval()\n",
        "    \n",
        "    for batch_num, (x_batch, y_batch) in zip(trange(len(test_loader)), test_loader):\n",
        "        pred = model(x_batch)\n",
        "        ce_loss = nn.CrossEntropyLoss()(pred, y_batch.squeeze()).item()\n",
        "        pred = torch.argmax(pred, dim = 1) #before: (bs, classes, h, w)\n",
        "        iou_sum = 0\n",
        "        pixel_acc_sum = 0\n",
        "        mean_acc_sum = 0\n",
        "        for i in range(x_batch.shape[0]):\n",
        "            iou_sum += mean_iou(pred[i].cpu().squeeze(), y_batch[i].cpu().squeeze())\n",
        "            pixel_acc_sum += pixel_accuracy(pred[i].cpu().squeeze(), y_batch[i].cpu().squeeze())\n",
        "            mean_acc_sum += mean_accuracy(pred[i].cpu().squeeze(), y_batch[i].cpu().squeeze())\n",
        "            \n",
        "        history[\"loss_val\"].append(ce_loss)\n",
        "        history[\"iou_val\"].append(iou_sum/x_batch.shape[0])\n",
        "        history[\"pixel_acc_val\"].append(pixel_acc_sum/x_batch.shape[0])\n",
        "        history[\"mean_acc_val\"].append(mean_acc_sum/x_batch.shape[0])\n",
        "        \n",
        "        if batch_num == 0:\n",
        "            plot_batch_with_results(x_batch.cpu(), y_batch.cpu(), pred.cpu()) \n",
        "            \n",
        "    for batch_num, (x_batch, y_batch) in zip(trange(len(train_loader)), train_loader):  \n",
        "        pred = torch.argmax(model(x_batch), dim = 1)\n",
        "        iou_sum = 0\n",
        "        pixel_acc_sum = 0\n",
        "        mean_acc_sum = 0\n",
        "        for i in range(x_batch.shape[0]):\n",
        "            pred_squeezed = pred[i].squeeze()\n",
        "            y_squeezed = y_batch[i].squeeze()\n",
        "            iou_sum += mean_iou(pred_squeezed, y_squeezed)\n",
        "            pixel_acc_sum += pixel_accuracy(pred_squeezed, y_squeezed)\n",
        "            mean_acc_sum += mean_accuracy(pred_squeezed, y_squeezed)\n",
        "        \n",
        "        history[\"iou_train\"].append(iou_sum/x_batch.shape[0])\n",
        "        history[\"pixel_acc_train\"].append(pixel_acc_sum/x_batch.shape[0])\n",
        "        history[\"mean_acc_train\"].append(mean_acc_sum/x_batch.shape[0])\n",
        "\n",
        "    return history\n",
        "    \n",
        "def train_procedure(model, opt, n_epochs, train_loader, test_loader, scheduler):\n",
        "    history = defaultdict(list)\n",
        "    steps = len(train_loader)\n",
        "\n",
        "    for epoch in range(n_epochs):\n",
        "        \n",
        "        train_loss = train_epoch(model, opt, train_loader)\n",
        "        test_history = test(model, test_loader, train_loader)\n",
        "        \n",
        "        history[\"loss_train\"].extend(train_loss)\n",
        "        history[\"loss_val\"].append((steps * (epoch + 1), np.mean(test_history[\"loss_val\"])))\n",
        "        \n",
        "        for key in test_history:\n",
        "            if (key != \"loss_val\"):\n",
        "                history[key].append(np.mean(test_history[key]))\n",
        "                \n",
        "        plot_history(history)\n",
        "        \n",
        "        print(\"Epoch average loss:\", np.mean(train_loss))\n",
        "        print(\"Epoch validation metrics:\")\n",
        "        print(\"\\tIoU -\", history['iou_val'][-1])\n",
        "        print(\"\\tmean accuracy -\", history['mean_acc_val'][-1])\n",
        "        print(\"\\tpixel acc -\", history[\"pixel_acc_val\"][-1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XPD_u5GYuk_h"
      },
      "source": [
        "Подберите параметры сети, обучите модель. Если все сделано правильно, то итоговое значение IoU должно превосходить 0.8."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ZmOq7ifuk_i"
      },
      "source": [
        "#your parameters\n",
        "n_classes = 2\n",
        "shape = (40,60)\n",
        "input_ch_size = 3\n",
        "hidden_ch_size = \n",
        "feature_levels_num = \n",
        "block_depth = \n",
        "\n",
        "batch_size = 8\n",
        "n_epochs = 25\n",
        "lr = 0.05\n",
        "lr_step = 10 \n",
        "lr_gamma = 0.5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bKviKokouk_q"
      },
      "source": [
        "train_dataset = WeizmannHorsesDataset(root=data_path, split=\"train\", img_shape = shape)\n",
        "test_dataset = WeizmannHorsesDataset(root=data_path, split=\"val\", img_shape = shape)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle = True)\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle = True)\n",
        "\n",
        "model = UNet(n_classes, feature_levels_num, input_ch_size, hidden_ch_size, block_depth)\n",
        "\n",
        "opt = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "scheduler = StepLR(opt, step_size=lr_step, gamma=lr_gamma)\n",
        "train_procedure(model, opt, n_epochs, train_loader, test_loader, scheduler = scheduler)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": true
        },
        "id": "m60O2Iahuk_t"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}